{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyznNkl6mTr7"
   },
   "source": [
    "### In order for Spark to talk to MongoDB, we need to initial the Spark context with pointers to the mongo uri and also include the mongo-spark-connector\n",
    "\n",
    "### Additionally, whoever configures the cluster may need to make sure additional jars are installed in $SPARK_HOME/jars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vM10f8dimTr9"
   },
   "outputs": [],
   "source": [
    "# This is sample code of how to create the Spark context with a custom configuration to connect to MongoDB\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.1 pyspark-shell'\n",
    "\n",
    "# def initspark(appname = \"Test\", servername = \"local\", mongo=\"mongodb://127.0.0.1/classroom\"):\n",
    "#     print ('initializing pyspark')\n",
    "#     conf = SparkConf().setAppName(appname).setMaster(servername)\n",
    "#     sc = SparkContext(conf=conf)\n",
    "#     spark = SparkSession.builder.appName(appname) \\\n",
    "#     .config(\"spark.mongodb.input.uri\", mongo) \\\n",
    "#     .config(\"spark.mongodb.output.uri\", mongo) \\\n",
    "#     .enableHiveSupport().getOrCreate()\n",
    "#     sc.setLogLevel(\"WARN\")\n",
    "#     print ('pyspark initialized')\n",
    "#     return sc, spark, conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdmgZyjXmTsC"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "#print(sys.version)\n",
    "os.environ[\"SPARK_HOME\"] = '/usr/local/spark'\n",
    "# os.environ[\"PYTHON_PATH\"] = 'python3'\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = 'python3'\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# PYSPARK_SUBMIT_ARGS is used to tell Spark to load additional packages when submitting the job.\n",
    "# In this case the mongo-spark-connector\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.1 pyspark-shell'\n",
    "sys.path.append('/class')\n",
    "from initspark import *\n",
    "sc, spark, conf = initspark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To read from a Mongo collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOjVg5_OmTsq"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.regions\").load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the query plan and what is sent to Mongo by running the following commands in a mongosh session\n",
    "```\n",
    "use Northwind\n",
    "db.setProfilingLevel(2)\n",
    "db.system.profile.find().sort({ts:-1}).limit(1).pretty()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, add a filter condition and see if it is pushed down to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter(\"RegionDescription='Western'\")\n",
    "df2.show()\n",
    "df2.explain()\n",
    "\n",
    "#db.system.profile.find().sort({ts:-1}).limit(1).pretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otXXWlKcmTst"
   },
   "source": [
    "### We can also take a DataFrame and write it to a Mongo destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4Qf4tu9mTsv"
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([('APAC', '5')])\n",
    "x1 = spark.createDataFrame(x, schema = ['RegionDescription', 'RegionID'])\n",
    "x1.write.format(\"mongo\").options(collection=\"regions\", database=\"Northwind\").mode(\"append\").save()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60inzyxbmTsy"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.regions\").load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Hs3nGamTs1"
   },
   "source": [
    "### Like any DataFrame, we can make it into a temporary view and use SparkSQL on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbLK9SC9mTs4"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('regions')\n",
    "spark.sql('select * from regions where regionid between 2 and 4').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here, we can start using Mongo collections just like tables from any other source, and use Spark to process them with SQL or Spark dot methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.categories\").load()\n",
    "p = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.products\").load()\n",
    "c.createOrReplaceTempView('categories')\n",
    "p.createOrReplaceTempView('products')\n",
    "spark.sql('''select c.categoryid, c.categoryname, p.productid, p.productname\n",
    "from products as p \n",
    "join categories as c on p.categoryid = c.categoryid \n",
    "order by c.categoryid, p.productid''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COLLECT_LIST can be used to create nested repeating fields instead of using the aggregate pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''select c.categoryid, c.categoryname\n",
    ", COLLECT_LIST(NAMED_STRUCT('productid', p.productid, 'productname', p.productname, 'unitprice', p.unitprice)) as products\n",
    "from products as p \n",
    "join categories as c on p.categoryid = c.categoryid \n",
    "group by c.categoryid, c.categoryname\n",
    "order by c.categoryid''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the SORT_ARRAY function lets you sort the contents of the nested collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyAE_C9VmTs8"
   },
   "outputs": [],
   "source": [
    "spark.sql('''select c.categoryid, c.categoryname\n",
    ", SORT_ARRAY(COLLECT_LIST(NAMED_STRUCT('productid', p.productid, 'productname', p.productname, 'unitprice', p.unitprice))) as products\n",
    "from products as p \n",
    "join categories as c on p.categoryid = c.categoryid \n",
    "group by c.categoryid, c.categoryname\n",
    "order by c.categoryid''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtyt9EvAmTs7"
   },
   "source": [
    "## LAB: ## \n",
    "### Write shippers to Mongo and find all the shippers with an 800 number using  a temporary view.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Unlike Cassandra, Mongo does not require a collection to exist before writing to it, so just write the DataFrame to a new collection\n",
    "<br>\n",
    "Make a DataFrame from the new Mongo collection and turn it into a temporary view\n",
    "<br>\n",
    "Use SQL-like expression to find the desired records\n",
    "<br>\n",
    "<br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "shippers.write.format(\"mongo\").options(collection=\"shippers\", database=\"classroom\").mode(\"append\").save()\n",
    "\n",
    "s=spark.read.format(\"mongo\").option(\"uri\",\"mongodb://127.0.0.1/classroom.shippers\").load()\n",
    "s.createOrReplaceTempView('shippers')\n",
    "display(spark.sql(\"select * from shippers where phone like '%800%'\"))\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB: ## \n",
    "### From the Northwind database, read products and suppliers and join them and display ProductID, ProductName, SupplierID, CompanyName, and Country. Limit the results to products from the USA.\n",
    "### Take a look at the query plan and the push down results with the following command in mongosh\n",
    "```\n",
    "db.system.profile.find().sort({ts:-1}).limit(2).pretty()\n",
    "```\n",
    "<br>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "s=spark.read.format(\"mongo\").option(\"uri\",\"mongodb://127.0.0.1/Northwind.suppliers\").load()\n",
    "s.createOrReplaceTempView('suppliers')\n",
    "p=spark.read.format(\"mongo\").option(\"uri\",\"mongodb://127.0.0.1/Northwind.products\").load()\n",
    "p.createOrReplaceTempView('products')\n",
    "\n",
    "j = spark.sql('''SELECT p.ProductID, p.ProductName, p.SupplierID, s.CompanyName, s.Country \n",
    "FROM products AS p\n",
    "JOIN suppliers AS s on p.SupplierID = s.SupplierID\n",
    "WHERE s.Country = 'USA'\n",
    "''')\n",
    "j.show()\n",
    "# db.system.profile.find().sort({ts:-1}).limit(2).pretty()\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "innQIsfGmTtB"
   },
   "source": [
    "## HOMEWORK: ## \n",
    "**First Challenge**\n",
    "\n",
    "Map the following Mongo tables in Northwind to temporary views: customers, orders, order-details, products & categories. \n",
    "\n",
    "**Second Challenge**\n",
    "\n",
    "Join all those tables and make another temporary view.\n",
    "\n",
    "**Third Challenge**\n",
    "\n",
    "Group the super join by OrderID, OrderDate, CustomerID, CompanyName and collect the order line items into a collection with the productid, productname, categoryid, categoryname, price paid, listed price, and quantity.\n",
    "\n",
    "**Fourth Challenge**\n",
    "\n",
    "Sort the collection by OrderID and save it to a new Mongo collection. \n",
    "\n",
    "**Bonus Challenge**\n",
    "\n",
    "Turn the previous collection into one that is grouped by CustomerID with the Orders under them and the order line items under the orders.\n",
    "<br>\n",
    "<p></p>\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "<br>\n",
    "**First Challenge**\n",
    "<br>\n",
    "Read each table from the NoSQL source and turn it into a temporary view.\n",
    "<br>\n",
    "```x = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/db.table\").load()\n",
    "x.createOrReplaceTempView('table1')\n",
    "```\n",
    "<br>\n",
    "<br>\n",
    "**Second Challenge**\n",
    "<br>\n",
    "Write a SQL query to join all the tables, common keys are CustomerID, OrderID, ProductID, CategoryID.\n",
    "<br>\n",
    "```orderjoin = spark.sql('''SELECT ....\n",
    "'''\n",
    ")\n",
    "```\n",
    "<br>\n",
    "<br>\n",
    "**Third Challenge**\n",
    "<br>\n",
    "Use SORT_ARRAY, COLLECT_LIST, NAMED_STRUCT with GROUP BY clause.\n",
    "<br>\n",
    "**Fourth Challenge**\n",
    "<br>\n",
    "```x.write.format(\"mongo\").options(collection=\"collection\", database=\"database\").mode(\"append\").save()\n",
    "```\n",
    "<br>\n",
    "<br>\n",
    "**Bonus Challenge**\n",
    "<br>\n",
    "Make the result of the nested join into a temporary view and do another COLLECT_LIST on CustomerID to group the orders into that.\n",
    "<br>\n",
    "</p>\n",
    "</details>\n",
    "<br>\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "cu = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.customers\").load()\n",
    "o = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.orders\").load()\n",
    "od = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.order-details\").load()\n",
    "p = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.products\").load()\n",
    "ca = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.categories\").load()\n",
    "\n",
    "cu.createOrReplaceTempView('customers')\n",
    "o.createOrReplaceTempView('orders')\n",
    "od.createOrReplaceTempView('orderdetails')\n",
    "p.createOrReplaceTempView('products')\n",
    "ca.createOrReplaceTempView('categories')\n",
    "\n",
    "orderjoin = spark.sql('''SELECT o.OrderID, o.OrderDate, o.CustomerID, cu.CompanyName\n",
    ", od.ProductID, od.UnitPrice as PurchasePrice, od.Quantity\n",
    ", p.ProductName, p.UnitPrice as ListPrice, ca.CategoryID, ca.CategoryName\n",
    "FROM orders AS o\n",
    "JOIN orderdetails AS od ON o.OrderID = od.OrderID\n",
    "JOIN products AS p ON od.ProductID = p.ProductID\n",
    "JOIN categories AS ca ON p.CategoryID = ca.CategoryID \n",
    "JOIN customers as cu ON o.CustomerID = cu.CustomerID\n",
    "''')\n",
    "orderjoin.createOrReplaceTempView('orderjoin')\n",
    "\n",
    "ord1 = spark.sql('''\n",
    "SELECT OrderID, OrderDate, CustomerID, CompanyName\n",
    ", SORT_ARRAY(COLLECT_LIST(NAMED_STRUCT('ProductID', ProductID, 'ProductName', ProductName\n",
    "               , 'CategoryID', CategoryID, 'CategoryName', CategoryName\n",
    "               , 'ListPrice', ListPrice, 'PurchasePrice', PurchasePrice, 'Quantity', Quantity\n",
    "               ))) AS LineItems\n",
    "\n",
    "from orderjoin\n",
    "GROUP BY OrderID, OrderDate, CustomerID, CompanyName\n",
    "''')\n",
    "\n",
    "#ord1.show()\n",
    "ord1.createOrReplaceTempView('ord1')\n",
    "# ord1.write.format(\"mongo\").options(collection=\"orders1\", database=\"Northwind2\").mode(\"append\").save()\n",
    "\n",
    "ord2 = spark.sql('''\n",
    "SELECT CustomerID, CompanyName\n",
    ", SORT_ARRAY(COLLECT_LIST(NAMED_STRUCT('OrderID', OrderID, 'OrderDate', OrderDate, 'LineItems', LineItems))) AS Orders\n",
    "from ord1\n",
    "GROUP BY CustomerID, CompanyName\n",
    "''')\n",
    "\n",
    "ord2.show()\n",
    "\n",
    "# ord2.write.format(\"mongo\").options(collection=\"orders1\", database=\"Northwind2\").mode(\"append\").save()\n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hmj4i8jEmTtC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
