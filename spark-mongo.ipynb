{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyznNkl6mTr7"
   },
   "source": [
    "### In order for Spark to talk to MongoDB we need to initial the spark context with pointers to the mongo uri and also include the mongo-spark-connector.\n",
    "\n",
    "### Additionally, whoever configures the cluster may need to make sure additional jars are installed in $SPARK_HOME/jars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vM10f8dimTr9"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.1 pyspark-shell'\n",
    "\n",
    "# def initspark(appname = \"Test\", servername = \"local\", mongo=\"mongodb://127.0.0.1/classroom\"):\n",
    "#     print ('initializing pyspark')\n",
    "#     conf = SparkConf().setAppName(appname).setMaster(servername)\n",
    "#     sc = SparkContext(conf=conf)\n",
    "#     spark = SparkSession.builder.appName(appname) \\\n",
    "#     .config(\"spark.mongodb.input.uri\", mongo) \\\n",
    "#     .config(\"spark.mongodb.output.uri\", mongo) \\\n",
    "#     .enableHiveSupport().getOrCreate()\n",
    "#     sc.setLogLevel(\"WARN\")\n",
    "#     print ('pyspark initialized')\n",
    "#     return sc, spark, conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdmgZyjXmTsC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n",
      "pyspark initialized\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "#print(sys.version)\n",
    "os.environ[\"SPARK_HOME\"] = '/usr/local/spark'\n",
    "# os.environ[\"PYTHON_PATH\"] = 'python3'\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = 'python3'\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.1 pyspark-shell'\n",
    "sys.path.append('/class')\n",
    "from initspark import *\n",
    "sc, spark, conf = initspark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOjVg5_OmTsq"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.regions\").load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otXXWlKcmTst"
   },
   "source": [
    "### We can also take a DataFrame and write it to a Mongo destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4Qf4tu9mTsv"
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([('APAC', '5')])\n",
    "x1 = spark.createDataFrame(x, schema = ['RegionDescription', 'RegionID'])\n",
    "x1.write.format(\"mongo\").options(collection=\"regions\", database=\"Northwind\").mode(\"append\").save()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60inzyxbmTsy"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.regions\").load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Hs3nGamTs1"
   },
   "source": [
    "### Like any DataFrame, we can make it into a temporary view and use SparkSQL on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbLK9SC9mTs4"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('regions')\n",
    "spark.sql('select * from regions where regionid between 2 and 4').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here we can start using Mongo collections just like tables from any other source, and use Spark to process them with SQL or Spark dot methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.categories\").load()\n",
    "p = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.products\").load()\n",
    "c.createOrReplaceTempView('categories')\n",
    "p.createOrReplaceTempView('products')\n",
    "spark.sql('''select c.categoryid, c.categoryname, p.productid, p.productname\n",
    "from products as p \n",
    "join categories as c on p.categoryid = c.categoryid \n",
    "order by c.categoryid, p.productid''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COLLECT_LIST can be used to create nested repeating fields instead of using the aggregate pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''select c.categoryid, c.categoryname\n",
    ", COLLECT_LIST(NAMED_STRUCT('productid', p.productid, 'productname', p.productname, 'unitprice', p.unitprice)) as products\n",
    "from products as p \n",
    "join categories as c on p.categoryid = c.categoryid \n",
    "group by c.categoryid, c.categoryname\n",
    "order by c.categoryid''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the SORT_ARRAY function let's you sort the contents of the nested collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyAE_C9VmTs8"
   },
   "outputs": [],
   "source": [
    "spark.sql('''select c.categoryid, c.categoryname\n",
    ", SORT_ARRAY(COLLECT_LIST(NAMED_STRUCT('productid', p.productid, 'productname', p.productname, 'unitprice', p.unitprice))) as products\n",
    "from products as p \n",
    "join categories as c on p.categoryid = c.categoryid \n",
    "group by c.categoryid, c.categoryname\n",
    "order by c.categoryid''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtyt9EvAmTs7"
   },
   "source": [
    "## LAB: ## \n",
    "### Write shippers to Mongo and find all the shippers with an 800 number using  a temporary view.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Unlike Cassandra, Mongo does not require a collection to exist before writing to it, so just write the DataFrame to a new collection\n",
    "<br>\n",
    "Make a DataFrame from the new Mongo collection and turn it into a temporary view\n",
    "<br>\n",
    "Use SQL-like expression to find the desired records\n",
    "<br>\n",
    "<br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "shippers.write.format(\"mongo\").options(collection=\"shippers\", database=\"classroom\").mode(\"append\").save()\n",
    "\n",
    "s=spark.read.format(\"mongo\").option(\"uri\",\"mongodb://127.0.0.1/classroom.shippers\").load()\n",
    "s.createOrReplaceTempView('shippers')\n",
    "display(spark.sql(\"select * from shippers where phone like '%800%'\"))\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "innQIsfGmTtB"
   },
   "source": [
    "## HOMEWORK: ## \n",
    "**First Challenge**\n",
    "\n",
    "Read Products from any source and write it to a Cassandra table. For simplicity, we only need to keep the productid, productname, and unitprice columns.\n",
    "\n",
    "**Second Challenge**\n",
    "\n",
    "Read Orders_LineItems.json from Day3 folder and write it to a Mongo collection.\n",
    "\n",
    "**Third Challenge**\n",
    "\n",
    "Join the Products and Orders_LineItems and join then, flatten them and regroup them so that the orders are grouped under each product instead.\n",
    "\n",
    "**Bonus Challenge**\n",
    "\n",
    "Include a calculated column showing how many times each product was ordered.\n",
    "\n",
    "A starting template has been provided in Day4-Homework.py to deal with preparing the Cassandra and Mongo environments for Challenges 1 & 2. If you have difficulty doing those on your own, then start with that template. Otherwise, try it from scratch from the code provided in the course so far.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "<br>\n",
    "Read each table from the NoSQL source and turn it into a temporary view\n",
    "<br>\n",
    "Use LATERAL VIEW EXPLODE() EXPLODED_TABLE to flatten out the nested format file or orders\n",
    "<br>\n",
    "Use the flattened results to join to products\n",
    "<br>\n",
    "Use the results of the join to group on productid, productname, and collect a structured list of customerid, orderid, orderdate, productid, quantity and price\n",
    "<br>\n",
    "Use the size function on the collected list to determine how many times each product was ordered or alternatively do it as part of the SQL query with other familiar techniques\n",
    "<br>\n",
    "<br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hmj4i8jEmTtC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|CustomerID|         CompanyName|              Orders|\n",
      "+----------+--------------------+--------------------+\n",
      "|     WOLZA|      Wolski  Zajazd|[[10374, 1996-12-...|\n",
      "|     MAISD|        Maison Dewey|[[10529, 1997-05-...|\n",
      "|     BLAUS|Blauer See Delika...|[[10501, 1997-04-...|\n",
      "|     MAGAA|Magazzini Aliment...|[[10275, 1996-08-...|\n",
      "|     FOLKO|      Folk och fä HB|[[10264, 1996-07-...|\n",
      "|     ANATR|Ana Trujillo Empa...|[[10308, 1996-09-...|\n",
      "|     ISLAT|      Island Trading|[[10315, 1996-09-...|\n",
      "|     VAFFE|        Vaffeljernet|[[10367, 1996-11-...|\n",
      "|     BLONP|Blondesddsl père ...|[[10265, 1996-07-...|\n",
      "|     CENTC|Centro comercial ...|[[10259, 1996-07-...|\n",
      "|     SPLIR|Split Rail Beer &...|[[10271, 1996-08-...|\n",
      "|     TRAIH|Trail's Head Gour...|[[10574, 1997-06-...|\n",
      "|     LILAS|   LILA-Supermercado|[[10283, 1996-08-...|\n",
      "|     WARTH|      Wartian Herkku|[[10266, 1996-07-...|\n",
      "|     FRANR| France restauration|[[10671, 1997-09-...|\n",
      "|     SEVES|  Seven Seas Imports|[[10359, 1996-11-...|\n",
      "|     EASTC|  Eastern Connection|[[10364, 1996-11-...|\n",
      "|     HILAA|    HILARION-Abastos|[[10257, 1996-07-...|\n",
      "|     HANAR|       Hanari Carnes|[[10250, 1996-07-...|\n",
      "|     DRACD|Drachenblut Delik...|[[10363, 1996-11-...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cu = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.customers\").load()\n",
    "o = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.orders\").load()\n",
    "od = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.order-details\").load()\n",
    "p = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.products\").load()\n",
    "ca = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/Northwind.categories\").load()\n",
    "\n",
    "cu.createOrReplaceTempView('customers')\n",
    "o.createOrReplaceTempView('orders')\n",
    "od.createOrReplaceTempView('orderdetails')\n",
    "p.createOrReplaceTempView('products')\n",
    "ca.createOrReplaceTempView('categories')\n",
    "\n",
    "orderjoin = spark.sql('''SELECT o.OrderID, o.OrderDate, o.CustomerID, cu.CompanyName\n",
    ", od.ProductID, od.UnitPrice as PurchasePrice, od.Quantity\n",
    ", p.ProductName, p.UnitPrice as ListPrice, ca.CategoryID, ca.CategoryName\n",
    "FROM orders AS o\n",
    "JOIN orderdetails AS od ON o.OrderID = od.OrderID\n",
    "JOIN products AS p ON od.ProductID = p.ProductID\n",
    "JOIN categories AS ca ON p.CategoryID = ca.CategoryID \n",
    "JOIN customers as cu ON o.CustomerID = cu.CustomerID\n",
    "''')\n",
    "orderjoin.createOrReplaceTempView('orderjoin')\n",
    "\n",
    "ord1 = spark.sql('''\n",
    "SELECT OrderID, OrderDate, CustomerID, CompanyName\n",
    ", SORT_ARRAY(COLLECT_LIST(NAMED_STRUCT('ProductID', ProductID, 'ProductName', ProductName\n",
    "               , 'CategoryID', CategoryID, 'CategoryName', CategoryName\n",
    "               , 'ListPrice', ListPrice, 'PurchasePrice', PurchasePrice, 'Quantity', Quantity\n",
    "               ))) AS LineItems\n",
    "\n",
    "from orderjoin\n",
    "GROUP BY OrderID, OrderDate, CustomerID, CompanyName\n",
    "''')\n",
    "\n",
    "#ord1.show()\n",
    "ord1.createOrReplaceTempView('ord1')\n",
    "\n",
    "ord2 = spark.sql('''\n",
    "SELECT CustomerID, CompanyName\n",
    ", SORT_ARRAY(COLLECT_LIST(NAMED_STRUCT('OrderID', OrderID, 'OrderDate', OrderDate, 'LineItems', LineItems))) AS Orders\n",
    "from ord1\n",
    "GROUP BY CustomerID, CompanyName\n",
    "''')\n",
    "\n",
    "ord2.show()\n",
    "\n",
    "ord1.write.format(\"mongo\").options(collection=\"orders1\", database=\"Northwind2\").mode(\"append\").save()\n",
    "ord2.write.format(\"mongo\").options(collection=\"orders1\", database=\"Northwind2\").mode(\"append\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
